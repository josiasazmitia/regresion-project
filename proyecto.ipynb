{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ciencia de Datos en Python - Proyecto #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proyecto consiste en aplicar los conocimientos aprendidos en clase (y apoyandose de referencias adicionales utiles) para crear modelos predictivos de regresi´on lineal uni-variable sencillos de la forma:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "y = β0 + β1 ∗ x\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "* y es la variable dependiente\n",
    "* x es la variable independiente\n",
    "* β0 es el intercepto de la recta\n",
    "* β1 es la pendiente de la recta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Usando sclicing con NumPy separar los datos en 2 datasets: entrenamiento(80 %) y validacion y pruebas(20 %)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Carga del archivo de entrenamiento\n",
    "data = np.load('./proyecto_training_data.npy')\n",
    "\n",
    "# buscamos obtner el 80% de la longitud del conjunto de datos\n",
    "train_size = int(len(data) * 0.8)\n",
    "\n",
    "# Utlizando slicing separamos el 80% de entrenamiento y 20% de validacion\n",
    "train_data = data[:train_size]\n",
    "validation_data = data[train_size:]\n",
    "\n",
    "#print(train_size)\n",
    "#print(validation_data)\n",
    "#validation_data\n",
    "\n",
    "print(f\"Conjunto de entrenamiento: {train_data.shape}\")\n",
    "print(f\"Conjunto de validación/pruebas: {validation_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Analisis exploratorio de datos: Para cada variable en el dataset calcular((usando numpy o pandas):\n",
    "* media\n",
    "* valor maximo\n",
    "* valor mınimo\n",
    "* rango(peak to peak, no el rango del tensor que por ser vector sabemos que es 1)\n",
    "* desviacion estandar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(data):\n",
    "    statistics = []\n",
    "    for i in range(data.shape[1]):\n",
    "        column = data[:, i][~np.isnan(data[:, i])]\n",
    "        mean = np.mean(column)  # Media\n",
    "        max_value = np.max(column)  # Valor máximo\n",
    "        min_value = np.min(column)  # Valor mínimo\n",
    "        range_value = np.ptp(column)  # Rango (peak to peak)\n",
    "        std_dev = np.std(column)  # Desviación estándar\n",
    "        \n",
    "        # Agregar las estadísticas calculadas a la lista\n",
    "        statistics.append((mean, max_value, min_value, range_value, std_dev))\n",
    "    return statistics\n",
    "\n",
    "# Calcular las estadísticas para los conjuntos de entrenamiento y de validación/pruebas\n",
    "train_statistics = calculate_statistics(train_data) # data entrenamiento 80%\n",
    "validation_statistics = calculate_statistics(validation_data) # data validacion 20%\n",
    "\n",
    "# Imprimir las estadísticas para el conjunto de entrenamiento\n",
    "print(\"Estadísticas del conjunto de entrenamiento:\")\n",
    "for i, stats in enumerate(train_statistics):\n",
    "    print(f\"Variable {i+1}: Media={stats[0]}, Máximo={stats[1]}, Mínimo={stats[2]}, Rango={stats[3]}, Desviación estándar={stats[4]}\")\n",
    "\n",
    "# Imprimir las estadísticas para el conjunto de validación/pruebas\n",
    "print(\"\\nEstadísticas del conjunto de validación/pruebas:\")\n",
    "for i, stats in enumerate(validation_statistics):\n",
    "    print(f\"Variable {i+1}: Media={stats[0]}, Máximo={stats[1]}, Mínimo={stats[2]}, Rango={stats[3]}, Desviación estándar={stats[4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Para cada variable en el dataset usar seaborn(funci´on distplot https://seaborn.pydata. org/generated/seaborn.distplot.html) para graficar un histograma de la variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignorar FutureWarnings específicos\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ['SalePrice', 'OverallQual', '1stFlrSF', 'TotRmsAbvGrd', 'YearBuilt', 'LotFrontage']\n",
    "variable_names = ['SalePrice', 'OverallQual', '1stFlrSF', 'TotRmsAbvGrd', 'YearBuilt', 'LotFrontage']\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "# Crear un histograma para cada variable\n",
    "for i, var_name in enumerate(variable_names):\n",
    "    plt.figure(figsize=(10, 6))  # Tamaño de la figura\n",
    "    # Filtrar NaNs y utilizar histplot para la visualización\n",
    "    sns.histplot(train_data[:, i][~np.isnan(train_data[:, i])], bins=30, kde=False, color='blue')\n",
    "    plt.title(f'Histograma de {var_name}')\n",
    "    plt.xlabel(var_name)\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Para cada variable independiente x :\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
